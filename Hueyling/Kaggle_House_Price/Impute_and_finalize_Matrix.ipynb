{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of features that are correlated with the SalePrice.\n",
    "Treat the NA as a category on it's own. If the feature is still important, it should show up.\n",
    "\n",
    "\n",
    "Couple features we want to make sure.\n",
    "MSSubClass -- appears to be number but it's actually categorical.\n",
    "\n",
    "https://emredjan.github.io/blog/2017/07/11/emulating-r-plots-in-python/#leverageplot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12 records within the test set are potentially problematics.\n",
    "- They are index 1556, 1916, 1946, 2121, 2152, 2189, 2217, 2251, 2474, 2490, 2577, 2905 in the final combined set.\n",
    "- They are Null in some fields which never existed in the train set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([1556, 1916, 1946, 2121, 2152, 2189, 2217, 2251, 2474, 2490, 2577, 2905], dtype='int64', name='Id')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "\n",
    "df_train = pd.read_csv('data/train.csv.gz', compression='gzip', header=0, sep=',', quotechar='\"')\n",
    "df_train_label = df_train[[\"Id\", \"SalePrice\"]]\n",
    "df_train = df_train.drop('SalePrice', axis=1)\n",
    "\n",
    "df_train = df_train.set_index(\"Id\")\n",
    "df_train_label = df_train_label.set_index(\"Id\")\n",
    "\n",
    "df_test = pd.read_csv('data/test.csv.gz', compression='gzip', header=0, sep=',', quotechar='\"')\n",
    "\n",
    "test_null_columns=df_test.columns[df_test.isnull().any()] \n",
    "train_null_columns=df_train.columns[df_train.isnull().any()] \n",
    "test_null_only_ColIdx = test_null_columns.difference(train_null_columns)\n",
    "\n",
    "test_null_only_RowIdx = [ df_test[df_test[idx].isnull()].index.tolist() for idx in test_null_only_ColIdx ]\n",
    "test_null_only_RowIdx = list ( set(x for l in test_null_only_RowIdx for x in l) )\n",
    "\n",
    "problematicTestSet = df_test.loc[ df_test.index.isin( test_null_only_RowIdx ) ]\n",
    "\n",
    "fineTestSet = df_test.loc[ ~df_test.index.isin( test_null_only_RowIdx ) ]  #1447 records\n",
    "\n",
    "problematicTestSet= problematicTestSet.set_index(\"Id\")\n",
    "fineTestSet = fineTestSet.set_index(\"Id\")\n",
    "df_test = df_test.set_index(\"Id\")\n",
    "\n",
    "df = pd.concat([df_train,df_test], axis=0, sort=True)\n",
    "\n",
    "problematicTestSet.index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a function to check out the records in the fields that we will merge and dummy them together. \n",
    "- Correct Exterior1st and Exterior2nd, Replace  :Brk Cmn  -> BrkComm and CmentBd  -> CemntBd\n",
    "- Check BsmtFinType1 and BsmtFinType2. Everything is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x_WdShing', 'x_BrkComm', 'x_CemntBd'}\n",
      "{'x_CmentBd', 'x_Brk Cmn', 'x_Wd Shng', 'x_Other'}\n",
      "set()\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "def checkWeirdExtra (inputDF, field1, field2):\n",
    "    var2_dummy_columns = pd.get_dummies(inputDF[field1], prefix= \"x\")\n",
    "    #print( len( var2_dummy_columns.columns ) )\n",
    "\n",
    "    var1_dummy_columns = pd.get_dummies(inputDF[field2], prefix= \"x\")\n",
    "    #print( len( var1_dummy_columns.columns ) )\n",
    "\n",
    "    var_dummy_columns = pd.concat([var1_dummy_columns,var2_dummy_columns],join='outer', sort=True).groupby(level=0).sum()\n",
    "    #print( len( var_dummy_columns.columns ) )\n",
    "\n",
    "    print ( set( var_dummy_columns.columns) - set( var1_dummy_columns.columns ) )\n",
    "    print ( set( var_dummy_columns.columns) - set( var2_dummy_columns.columns ) )\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "checkWeirdExtra(df, \"Exterior1st\", \"Exterior2nd\")\n",
    "\n",
    "df.Exterior1st = df.Exterior1st.str.replace(\"Brk Cmn\", \"BrkComm\")\n",
    "df.Exterior2nd = df.Exterior2nd.str.replace(\"Brk Cmn\", \"BrkComm\")\n",
    "df.Exterior1st = df.Exterior1st.str.replace(\"CmentBd\", \"CemntBd\")\n",
    "df.Exterior2nd = df.Exterior2nd.str.replace(\"CmentBd\", \"CemntBd\")\n",
    "df.Exterior1st = df.Exterior1st.str.replace(\"Wd Shng\", \"WdShing\")\n",
    "df.Exterior2nd = df.Exterior2nd.str.replace(\"Wd Shng\", \"WdShing\")\n",
    "\n",
    "checkWeirdExtra(df, \"BsmtFinType1\", \"BsmtFinType2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start to purposely encode the information based on our best understanding.\n",
    "- Combine Exterior1st and Exterior2nd to **Exterior**\n",
    "- BsmtFinType1 and BsmtFinType2 to **Bsmt** \n",
    "    -Replace each type to it's actually square feet  BsmtFinSF1, BsmtFinSF2\n",
    "    -For Unf in type 1 and type2, replace it with the BsmtUnfSF\n",
    "- Combine BsmtFullBath, BsmtHalfBath to **BsmtBath**\n",
    "- Add all different PorchSF to **TotalProchSF**\n",
    "- Dummy MasVnrType to **MasVnr** and replace the value with MasVnrArea\n",
    "- Dummy PoolQC to **Pool** and replace the value with PoolArea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purposelyEncodeData(inputDF):\n",
    "    \n",
    "    preProcessCatField = [\"MasVnrType\", \"Exterior1st\", \"Exterior2nd\", \"BsmtFinType1\", \"BsmtFinType2\"]\n",
    "    preProcessNumFiled = [\"MasVnrArea\", \"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\", \"BsmtFullBath\", \"BsmtHalfBath\"]\n",
    "    \n",
    "    inputDF[preProcessCatField] = inputDF[preProcessCatField].fillna(\"Unknown\")\n",
    "    inputDF[preProcessNumFiled] = inputDF[preProcessNumFiled].fillna(-1)\n",
    "    \n",
    "    # Need to encode some fields based on their definition and drop the original.\n",
    "    # Exterior1st, Exterior2nd (Exterior covering on house)\n",
    "\n",
    "    var1_dummy_columns = pd.get_dummies(inputDF['Exterior1st'], prefix= \"Exterior\")\n",
    "    var2_dummy_columns = pd.get_dummies(inputDF['Exterior2nd'], prefix= \"Exterior\")\n",
    "    var_dummy_columns = pd.concat([var1_dummy_columns,var2_dummy_columns], join='outer', sort=True).groupby(level=0).sum()\n",
    "    var_dummy_columns = var_dummy_columns.replace(2, 1) \n",
    "    \n",
    "    #for k, v in var1_dummy_columns.nunique().to_dict().items():\n",
    "    #    print('{}={}'.format(k,v))\n",
    "\n",
    "    inputDF = pd.concat([inputDF, var_dummy_columns], join='outer', sort=True, axis=1)\n",
    "    inputDF = inputDF.drop( columns=['Exterior1st', 'Exterior2nd'] )\n",
    "    print( inputDF.shape )\n",
    "\n",
    "    # BsmtFinType1, BsmtFinType2, BsmtFinSF1 (Type 1 finished square feet), BsmtFinSF2 (Type 1 finished square feet), BsmtUnfSF: Unfinished square feet of basement area\n",
    "    # TotalBsmtSF: Total square feet of basement area keep\n",
    "    var1_dummy_columns = pd.get_dummies(inputDF['BsmtFinType1'], prefix= \"Bsmt\") \n",
    "    var1_dummy_columns = var1_dummy_columns.mul( inputDF['BsmtFinSF1'] , axis=0)\n",
    "    tmp = var1_dummy_columns['Bsmt_Unf']\n",
    "    tmp [ inputDF.loc[inputDF['BsmtFinType1'] == \"Unf\"].index ] = 1\n",
    "    tmp = tmp.mul( inputDF['BsmtUnfSF'] , axis=0)\n",
    "    var1_dummy_columns['Bsmt_Unf'] = tmp\n",
    "\n",
    "    var2_dummy_columns = pd.get_dummies(inputDF['BsmtFinType2'], prefix= \"Bsmt\") \n",
    "    var2_dummy_columns = var2_dummy_columns.mul( inputDF['BsmtFinSF2'] , axis=0)\n",
    "    tmp = var2_dummy_columns['Bsmt_Unf']\n",
    "    tmp [ inputDF.loc[inputDF['BsmtFinType2'] == \"Unf\"].index ] = 1\n",
    "    tmp = tmp.mul( inputDF['BsmtUnfSF'] , axis=0)\n",
    "    var2_dummy_columns['Bsmt_Unf'] = tmp\n",
    "    \n",
    "    for i, v in var1_dummy_columns['Bsmt_Unf'].items():\n",
    "        if v > 0:\n",
    "            var2_dummy_columns['Bsmt_Unf'].loc[i] = 0\n",
    "\n",
    "    var_dummy_columns = pd.concat([var1_dummy_columns,var2_dummy_columns], join='outer', sort=True).groupby(level=0).sum()\n",
    "\n",
    "    #for k, v in var_dummy_columns.nunique().to_dict().items():\n",
    "    #    print('{}={}'.format(k,v))\n",
    "\n",
    "    inputDF = pd.concat([inputDF, var_dummy_columns], join='outer', sort=True, axis=1)\n",
    "    inputDF = inputDF.drop( columns=['BsmtFinType1', 'BsmtFinType1', 'BsmtFinType2', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF'] )\n",
    "    print( inputDF.shape )\n",
    "\n",
    "    #BsmtFullBath, BsmtHalfBath  (number of type of bathroom in the basement)\n",
    "    inputDF['BsmtBath'] = inputDF[\"BsmtFullBath\"] + 0.5* inputDF[\"BsmtHalfBath\"] \n",
    "    inputDF = inputDF.drop( columns=['BsmtFullBath', 'BsmtHalfBath'] )\n",
    "    print( inputDF.shape )\n",
    "\n",
    "    #OpenPorchSF ( Open porch area in square feet)\n",
    "    #EnclosedPorch (Enclosed porch area in square feet)\n",
    "    #3SsnPorch (Three season porch area in square feet)\n",
    "    #ScreenPorch (Screen porch area in square feet)\n",
    "    inputDF[\"TotalProchSF\"] = inputDF[\"OpenPorchSF\"] + inputDF[\"EnclosedPorch\"] + inputDF[\"3SsnPorch\"] + inputDF[\"ScreenPorch\"] \n",
    "    print( inputDF.shape )\n",
    "\n",
    "    #MasVnrType, MasVnrArea\n",
    "    var_dummy_columns = pd.get_dummies(inputDF['MasVnrType'], prefix= \"MasVnr\") \n",
    "    var_dummy_columns = var_dummy_columns.mul( inputDF['MasVnrArea'] , axis=0)\n",
    "    inputDF = pd.concat([inputDF, var_dummy_columns], join='outer', sort=True, axis=1)\n",
    "    inputDF = inputDF.drop( columns=['MasVnrType', 'MasVnrArea'] )\n",
    "    print( inputDF.shape )\n",
    "\n",
    "    return (inputDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2919, 94)\n",
      "(2919, 96)\n",
      "(2919, 95)\n",
      "(2919, 96)\n",
      "(2919, 99)\n"
     ]
    }
   ],
   "source": [
    "new = purposelyEncodeData(df.copy())\n",
    "new.MSSubClass = new.MSSubClass.astype('object')\n",
    "# new.OverallCond and new.OverallQual is already encoded. So, don't have to do anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([333, 2121], dtype='int64', name='Id')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new[ new[\"Bsmt_Unknown\"] != 0 ].index\n",
    "## index = 333, BsmtFinType2 is unknown but it has square feet for type2, we keep this as unknown b/c we don't know \n",
    "## index = 2121 Nan for all the BsmtFinType series.\n",
    "## The cleaning should be techinically correct.\n",
    "#df.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the level of each categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alley 2\n",
      "BldgType 5\n",
      "BsmtCond 4\n",
      "BsmtExposure 4\n",
      "BsmtQual 4\n",
      "CentralAir 2\n",
      "Condition1 9\n",
      "Condition2 8\n",
      "Electrical 5\n",
      "ExterCond 5\n",
      "ExterQual 4\n",
      "Fence 4\n",
      "FireplaceQu 5\n",
      "Foundation 6\n",
      "Functional 7\n",
      "GarageCond 5\n",
      "GarageFinish 3\n",
      "GarageQual 5\n",
      "GarageType 6\n",
      "Heating 6\n",
      "HeatingQC 5\n",
      "HouseStyle 8\n",
      "KitchenQual 4\n",
      "LandContour 4\n",
      "LandSlope 3\n",
      "LotConfig 5\n",
      "LotShape 4\n",
      "MSSubClass 16\n",
      "MSZoning 5\n",
      "MiscFeature 4\n",
      "Neighborhood 25\n",
      "PavedDrive 3\n",
      "PoolQC 3\n",
      "RoofMatl 8\n",
      "RoofStyle 6\n",
      "SaleCondition 6\n",
      "SaleType 9\n",
      "Street 2\n",
      "Utilities 2\n"
     ]
    }
   ],
   "source": [
    "for c in new.columns:\n",
    "    if new[c].dtype == 'object':\n",
    "        print(c, len(new[c].value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordinal Features\n",
    "Label Count encoding is good in general, however, some of the features are ordinal in nature.\n",
    "For example, we usually consider Excellent > Good > Average/Typical > Fair > Poor\n",
    "We can construct a dictionary like the following and map it to those columns:\n",
    "\n",
    "{'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa':2, 'Po':1}\n",
    "You need a different dictionary for columns with different levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_cols = ['ExterQual', 'ExterCond','BsmtCond','HeatingQC', 'KitchenQual', \n",
    "           'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC']\n",
    "ord_dic = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa':2, 'Po':1}\n",
    "\n",
    "ord_df = new.copy()\n",
    "for col in ord_cols:\n",
    "    ord_df[col] = ord_df[col].map(lambda x: ord_dic.get(x, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = ord_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Count encoding or One-hot encoding ?\n",
    "- Rank categorical variables by count in the **training** set and transform the test set\n",
    "- Iterate counter for each CV fold - fit on the **new training set** and transform on the **new test set**\n",
    "- Useful for both linear or non-linear algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelCountEncoder(object):\n",
    "    def __init__(self):\n",
    "        self.count_dict = {}\n",
    "        self.rev_count_dict = {}\n",
    "        \n",
    "    def fit(self, column):\n",
    "        # We want to rank the key by its value and use the rank as the new value\n",
    "        count = column.value_counts()\n",
    "        self.count_dict = dict( list( zip (count.index, reversed(range(len(count)+1 ) ) ) ) ) \n",
    "        self.rev_count_dict = dict( list( zip ( reversed(range(len(count)+1 ) ) , count.index ) ) ) \n",
    "            \n",
    "    def transform(self, column):\n",
    "        # If a category only appears in the test set, we will assign the value to zero.\n",
    "        missing = 0\n",
    "        return column.map(lambda x: self.count_dict.get(x, missing))\n",
    "              \n",
    "    def fit_transform(self, column):\n",
    "        self.fit(column)\n",
    "        return self.transform(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2919, 99)\n"
     ]
    }
   ],
   "source": [
    "df_label_count = new.copy()\n",
    "encodedDic = {}\n",
    "\n",
    "for i, c in enumerate ( df_label_count.columns ):\n",
    "    if df_label_count[c].dtype == 'object':\n",
    "        lce = LabelCountEncoder()\n",
    "        df_label_count[c] = lce.fit_transform(df_label_count[c])\n",
    "        encodedDic[df_label_count.columns[i]] = lce.rev_count_dict\n",
    "    else:\n",
    "        df_label_count[c] = df_label_count[c].fillna(-1)\n",
    "print( df_label_count.shape )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2919, 181)\n",
      "(2919, 69)\n",
      "(2919, 250)\n"
     ]
    }
   ],
   "source": [
    "df_onehot = new.copy()\n",
    "object_feats = df_onehot.dtypes[df_onehot.dtypes == \"object\"].index\n",
    "numeric_feats = df_onehot.dtypes[df_onehot.dtypes != \"object\"].index\n",
    "objEnc = pd.get_dummies(df_onehot[object_feats], drop_first=True, dummy_na=True)\n",
    "print( objEnc.shape )\n",
    "numEnc = df_onehot[numeric_feats].fillna(-1)\n",
    "print( numEnc.shape )\n",
    "df_onehot = pd.concat( [objEnc,numEnc] , axis=1, join='outer', sort=True)\n",
    "print( df_onehot.shape )\n",
    "#df_onehot.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2919, 211)\n",
      "(2919, 69)\n",
      "(2919, 280)\n"
     ]
    }
   ],
   "source": [
    "df_onehot_nodrop = new.copy()\n",
    "object_feats = df_onehot_nodrop.dtypes[df_onehot_nodrop.dtypes == \"object\"].index\n",
    "numeric_feats = df_onehot_nodrop.dtypes[df_onehot_nodrop.dtypes != \"object\"].index\n",
    "objEnc = pd.get_dummies(df_onehot_nodrop[object_feats], drop_first=False, dummy_na=True)\n",
    "print( objEnc.shape )\n",
    "numEnc = df_onehot_nodrop[numeric_feats].fillna(-1)\n",
    "print( numEnc.shape )\n",
    "df_onehot_nodrop = pd.concat( [objEnc,numEnc] , axis=1, join='outer', sort=True)\n",
    "print( df_onehot_nodrop.shape )\n",
    "#df_onehot.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the label encoded processed dataframes to the pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_label_count\n",
    "df_test = df[df.index >= min(df_test.index)]  ## determine if it is a test set or not. Have to encode all of them together.\n",
    "df_train = df[df.index < min(df_test.index)]\n",
    "\n",
    "df_train.to_pickle(\"df_train.pkl\")\n",
    "df_train_label.to_pickle(\"df_train_label.pkl\")\n",
    "df_test.to_pickle(\"df_test.pkl\")\n",
    "\n",
    "import pickle\n",
    "with open('dic.pkl', 'wb') as handle:\n",
    "    pickle.dump(encodedDic, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#with open('dic.pkl', 'rb') as handle:\n",
    "#    savedEncodeDic = pickle.load(handle)\n",
    "#print (encodedDic == savedEncodeDic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the onehot encoded processed dataframes to the pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_onehot\n",
    "df_test = df[df.index >= min(df_test.index)]  ## determine if it is a test set or not. Have to encode all of them together.\n",
    "df_train = df[df.index < min(df_test.index)]\n",
    "\n",
    "df_train.to_pickle(\"df_train_onehot.pkl\")\n",
    "df_train_label.to_pickle(\"df_train_onehot_label.pkl\")\n",
    "df_test.to_pickle(\"df_test_onehot.pkl\")\n",
    "\n",
    "\n",
    "df = df_onehot_nodrop\n",
    "df_test = df[df.index >= min(df_test.index)]  ## determine if it is a test set or not. Have to encode all of them together.\n",
    "df_train = df[df.index < min(df_test.index)]\n",
    "\n",
    "df_train.to_pickle(\"df_train_onehot_nodrop.pkl\")\n",
    "df_train_label.to_pickle(\"df_train_onehot_nodrop_label.pkl\")\n",
    "df_test.to_pickle(\"df_test_onehot_nodrop.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 280)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
